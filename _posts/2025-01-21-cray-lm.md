## Introducing Cray-LM

Today I am excited to release Cray-LM.

Cray-LM is a fully open source, Apache 2 Licensed, integrated LLM inference and training platform.

* Contact us for help deploying Cray-LM on your GPU supercomputer: [Contact](https://forms.gle/tk6LFVrTQDSQp8L69)
* Access the source code at: [Github](https://github.com/supermassive-intelligence/cray)
* Read the documentation and examples at: [Docs](https://docs.smasint.com)

Cray builds on top of the vLLM inference engine, the Megatron-LM training framework, and the HuggingFace model hub. It unifies the capabilities of these tools into a single platform, enabling users to easily perform LLM inference and training, and build higher lever applications such as LLM-Agents.

Cray is designed for high peformance. It inherits the distributed training capabilities of Megatron-LM and the optimized inference engine of vLLM. Cray is also designed to be easy to use. It provides an OpenAI compatible server and a simple command line interface for users to interact with the platform.

Cray-LM is inspired by the work of Seymour Roger Cray (September 28, 1925 â€“ October 5, 1996), an American electrical engineer and supercomputer architect who designed a series of computers that were the fastest in the world for decades, and founded Cray Research, which built many of these machines. Called "the father of supercomputing", Cray has been credited with creating the supercomputer industry.

### Lessons Learned

Over the last ten years, I've built seven generations of training and inference
stacks at Baidu, AI Fund, MLCommons, Lamini, and multiple enterprise
partners. I've learned a lot about what works and what doesn't. Here are some
of the key lessons I've learned:

* Training and Inference are both essential, but most frameworks focus on one or the other.
* SLURM and Kubernetes have different worldviews, but pieces of both are useful.
* A model exchange format has not yet emerged, and the best we have is the huggingface model hub.
* The main dependency of Deep Learning is the GPU, and it it possible to cut out the rest.

### Unified Training and Inference

Training is essential because it produces and updates models. A machine learning model without training
could never be updated and could not learn.

Inference is essential for models to have practical value. Inference is used when a model is deployed in
production to serve predictions to users.

Cray-LM unifies training and inference into a single platform. This enables users to simultaneously train
and deploy models, and to easily switch between the two modes without changing their code.

