## Introducing Cray-LM

Today I am excited to release Cray-LM.

Cray-LM is a fully open source, Apache 2 Licensed, integrated LLM inference and training platform.

* Contact us for help deploying Cray-LM on your GPU supercomputer: [Contact](https://forms.gle/tk6LFVrTQDSQp8L69)
* Access the source code at: [Github](https://github.com/supermassive-intelligence/cray)
* Read the documentation and examples at: [Docs](https://docs.smasint.com)

Cray builds on top of the vLLM inference engine, the Megatron-LM training framework, and the HuggingFace model hub. It unifies the capabilities of these tools into a single platform, enabling users to easily perform LLM inference and training, and build higher lever applications such as LLM-Agents.

Cray is designed for high peformance. It inherits the distributed training capabilities of Megatron-LM and the optimized inference engine of vLLM. Cray is also designed to be easy to use. It provides an OpenAI compatible server and a simple command line interface for users to interact with the platform.

Cray-LM is inspired by the work of Seymour Roger Cray (September 28, 1925 â€“ October 5, 1996), an American electrical engineer and supercomputer architect who designed a series of computers that were the fastest in the world for decades, and founded Cray Research, which built many of these machines. Called "the father of supercomputing", Cray has been credited with creating the supercomputer industry.

We hope that Cray-LM will help to democratize access to large language models, and enable more people to build and deploy AI systems that benefit society. We are excited to see what you will build with Cray-LM!

If you are interested in contributing to Cray-LM, please reach out to us at [Contact](https://forms.gle/tk6LFVrTQDSQp8L69).

### Lessons Learned

Over the last ten years, we have built seven generations of training and inference
stacks at Baidu, AI Fund, MLCommons, Lamini, and multiple enterprise
partners. We have learned a lot about what works and what doesn't. Here are some
of the key lessons learned:

* Training and Inference are both essential, but most frameworks focus on one or the other.
* SLURM and Kubernetes have different worldviews, but pieces of both are useful.
* A model exchange format has not yet emerged, and the best we have is the huggingface model hub.
* The main dependency is the GPU, and it it possible to cut out the rest.

These infrastructure issues are so difficult that many organizations focus on either training or inference,
leaving significant accuracy on the table. Our vision for Cray-LM is that will enable organizations to build
LLM systems that leverage backpropagation in addition to in-context learning, enabling them to solve more
challenging problems.

### Unified Training and Inference

Training is essential because it produces and updates models. A machine learning model without training
could never be updated and could not learn.

Inference is essential for models to have practical value. Inference is used when a model is deployed in
production to serve predictions to users.

Cray-LM unifies training and inference into a single platform. This enables users to simultaneously train
and deploy models, and to easily switch between the two modes without changing their code.

It does this by building on pytorch models stored in the huggingface model hub. This allows training models
that are supported by huggingface. The inference engine is based on vLLM, which loads the saved huggingface
models. It is possible to train new models from scratch, or to continue training existing models.

### SLURM and Kubernetes are Better Together

SLURM is a batch job scheduler that is widely used in high performance computing environments. It is designed to
run large numbers of jobs on a cluster of machines with a high performance network. The high performance network
is essential for training large models, because it allows the GPUs to communicate with each other quickly using RDMA
primitive operations that are composed into collective operations like allreduce and allgather. Superficially it
seems like SLURM only handles job scheduling, but it also sets up the MPI environment that brings up the high performance
network that distributed training frameworks like Megatron-LM tap into.

Kubernetes is a container orchestration system that is widely used in cloud computing environments. It handles the
problem of deploying containers onto a cluster of machines. Containers are essential to manage the complex software
dependencies. If you want to upgrade the software on the cluster, you can just upgrade the container image and
restart the container. Kubernetes allows you to do this without affecting the other containers running on the cluster.

SLURM is a set of lightweight C programs that coordinate over TCP sockets, and tap into rdma interfaces exposed by linux network
drivers. It is possible to entirely encapsulate SLURM in a container, and run it on Kubernetes.

### Huggingface is the model exchange format

The huggingface model hub is the closest thing we have to a model exchange format. The transformers library has pytorch source
code implementations of hundreds of classes of models. Even though most LLMs are based on transformers, there is no standard
implementation of a transformer. Different models might choose different position embeddings or layer shapes. One of the biggest
points of friction between training and inference teams in large companies, is resolving these differences. Cray-LM modifies Megraton-LM
to use the huggingface model hub as the source of truth for model implementations, which makes it possible to immediately load most trained
models directly into the inference engine.

### Cutting out everything except the GPU

Multiple previous generations of Cray-LM used a variety of cloud services including
parallel distributed object storage such as Azure Blob or GCS, database services like
BigQuery, cloud load balancers. These lead to significant complexity and reduced portability
between supercomputers, which are often optimized for efficiency and have different cloud stacks.
The current generation of Cray-LM requires GPU compute nodes, a high performance interconnect,
and nothing else.

We had develop several design issues to make this possible. The first is that the training jobs
need to produce a model that is passed to the inference engine and state needs to be tracked between
the training and inference workers.

### Future

We plan to keep Cray-LM up to date with the latest advances in LLM research, including new models,
new training stack optimizations, and new inference stack optimizations.

We are excited to build a new generation of LLM supercomputers that democratize access to
training language models. We are in particular interested in applications that build on
top of Cray-LM to support LLM agents that train and deploy themselves.


