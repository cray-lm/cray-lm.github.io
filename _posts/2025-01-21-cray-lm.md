## Introducing Cray-LM

Today I am excited to release Cray-LM version 0.5

Cray-LM is a fully open source, Apache 2 Licensed, integrated LLM inference and training platform.

* Contact us for help deploying Cray-LM on your GPU supercomputer: [Contact](https://forms.gle/tk6LFVrTQDSQp8L69)
* Access the source code at: [Github](https://github.com/supermassive-intelligence/cray)
* Read the documentation and examples at: [Docs](https://docs.smasint.com)

Cray builds on top of the vLLM inference engine, the Megatron-LM training framework, and the HuggingFace model hub. It unifies the capabilities of these tools into a single platform, enabling users to easily perform LLM inference and training, and build higher level applications such as LLM-Agents.

Cray is designed for high peformance. It inherits the distributed training capabilities of Megatron-LM and the optimized inference engine of vLLM. Cray is also designed to be easy to use. It provides an OpenAI compatible server and a simple command line interface for users to interact with the platform.

Cray-LM is inspired by the work of Seymour Roger Cray (September 28, 1925 â€“ October 5, 1996), an American electrical engineer and supercomputer architect who designed a series of computers that were the fastest in the world for decades, and founded Cray Research, which built many of these machines. Called "the father of supercomputing", Cray has been credited with creating the supercomputer industry.

We hope that Cray-LM will help to democratize access to large language models, and enable more people to build and deploy AI systems that benefit society. We are excited to see what you will build with Cray-LM!

If you are interested in contributing to Cray-LM, please reach out to us at [Contact](https://forms.gle/tk6LFVrTQDSQp8L69).

### Lessons Learned

Over the last ten years, we have built seven generations of training and inference
stacks at Baidu, AI Fund, MLCommons, Lamini, and multiple enterprise
partners. We have learned a lot about what works and what doesn't. Here are some
of the key lessons learned:

* Training and Inference are both essential, but most frameworks focus on one or the other.
* SLURM and Kubernetes have different worldviews, but pieces of both are useful.
* A model exchange format has not yet emerged, and the best we have is the huggingface model hub.
* The main dependency is the GPU, and it it possible to achieve a simpler and more secure design by cutting out the rest.

These infrastructure issues are so difficult that many organizations focus on either training or inference,
leaving significant accuracy on the table. Our vision for Cray-LM is to enable organizations to build
LLM systems that leverage backpropagation in addition to in-context learning, just like humans,
enabling them to solve more challenging problems.

### Unified Training and Inference

Training is essential because it produces and updates models. A machine learning model without training
could never be updated and could not learn.

Inference is essential for models to have practical value. Inference is used when a model is deployed in
production to serve predictions to users.

Cray-LM unifies training and inference into a single platform. This enables users to simultaneously train
and deploy models, and to easily switch between the two modes without changing their code.

It does this by building on pytorch models stored in the huggingface model hub. This allows training models
that are supported by huggingface. The inference engine is based on vLLM, which loads the saved huggingface
models. It is possible to train new models from scratch, or to continue training existing models.

A Cray-LM cluster will spin up inference and training workers as needed. Training jobs are submitted to the
SLURM queue and pulled out by the training workers as batch jobs. Inference requests are submitted to a persistent
queue and pulled out by the inference workers as needed. As checkpoints are produced by the training workers, they
are pickup up by the inference workers and new inference requests can immediately use the new model.

### SLURM and Kubernetes are Better Together

Many software developers are familiar with Kubernetes for cloud and enterprise apps. So the natural temptation is to
use Kubernetes as the job queue, e.g. with the MPI Operator to get the network to work. This leaves out optimized
scheduling that handles the network locality that exists in GPU RDMA clusters but not cloud VMs, and it forces the
user to set up the RDMA network stack manually, which is so complicated it usually doesn't work and or requires
significant customization.

Kubernetes without SLURM can work, but it forces the infra team to reinvent the wheel.  

Some HPC clusters run SLURM on bare metal. This also works and simplifies the network configuration, but deployments
without containers create version and dependency management hell. 

In Cray-LM, we run a virtual SLURM cluster inside of a kubernetes application, providing the best of both worlds. We
are closely monitoring development on the [SLINKY](https://slurm.schedmd.com/slinky.html) project and expect to fold
in better approaches as they become available.

Let's look at SLURM and Kubernetes in more detail.

SLURM is a batch job scheduler that is widely used in high performance computing environments. It is designed to
run large numbers of jobs on a cluster of machines with a high performance network. The high performance network
is essential for training large models, because it allows the GPUs to communicate with each other quickly using RDMA
primitive operations that are composed into collective operations like allreduce and allgather. Superficially it
seems like SLURM only handles job scheduling, but it also sets up the MPI environment that brings up the high performance
network that distributed training frameworks like Megatron-LM tap into.

Kubernetes is a container orchestration system that is widely used in cloud computing environments. It handles the
problem of deploying containers onto a cluster of machines. Containers are essential to manage the complex software
dependencies. If you want to upgrade the software on the cluster, you can just upgrade the container image and
restart the container. Kubernetes allows you to do this without affecting the other containers running on the cluster.

SLURM is a set of lightweight C programs that coordinate over TCP sockets, and tap into rdma interfaces exposed by linux network
drivers. It is possible to entirely encapsulate SLURM in a container, and run it on Kubernetes.

### Huggingface is the model exchange format

The huggingface model hub is the closest thing we have to a model exchange format. The transformers library has pytorch source
code implementations of hundreds of classes of models. Even though most LLMs are based on transformers, there is no standard
implementation of a transformer. Different models might choose different position embeddings or layer shapes. One of the biggest
points of friction between training and inference teams in large companies, is resolving these differences. Cray-LM modifies Megraton-LM
to use the huggingface model hub as the source of truth for model implementations, which makes it possible to immediately load most trained
models directly into the inference engine.

### Cutting out everything except the GPU

Multiple previous generations of Cray-LM used a variety of cloud services including
parallel distributed object storage such as Azure Blob or GCS, database services like
BigQuery, cloud load balancers. These lead to significant complexity and reduced portability
between supercomputers, which are often optimized for efficiency and have different cloud stacks.
The current generation of Cray-LM requires GPU compute nodes, a high performance interconnect,
and nothing else.

We had to address several design issues to make this possible.

#### Training State Versioning without a Database or Object Store

The first is that the training jobs need to produce a model that is passed to
the inference engine and state needs to be tracked between the training and inference workers.
We uniquely identify each model by a hash of the training data, the model code, and the
hyperparameters. This state is saved to a shared filesystem that is accessible by both the
training and inference workers. The training system picks up this training state and launches
a SLURM job to train the model. As it trains, it produces checkpoints that are saved to the
shared filesystem. The inference system picks up these checkpoints as they are made available. This enables
fault tolerance, because if a training worker fails, training system can restart from a checkpoint. It also avoids
the need for a database to track the state of the training jobs.

#### Massive Inference without Cloud Queues or Load Balancers

LLMs are big and expensive, even on GPUs. Generating thousands of tokens can take minutes. A user who submits a few
requests from their laptop in a loop can quickly overwhelm a GPU cluster that costs millions of dollars. Submitting
these requests as HTTP REST requests leads to load imbalance and timeouts. Submitting too few requests leads to
underutilization. vLLM uses continuous batching to handle load imbalance, but that can still lead to overwhelming
the workers. Cray-LM pushes inference requests into a persistent queue, which can be very large in size. The requests
are pulled out by vLLM workers as they have capacity. The user client polls to get the results from the queue. This
allows efficient and lossless processing of very high numbers of inference requests, common to inference pipelines.

#### Datasets without an Object Store

Training requires training data. Typically this requires uploading a training dataset. The natural place to 
put it is in a cloud object store. It initially seems hard to get around this. But how hard it is really?

How big is each data item that an LLM is trained on? A few hundred tokens.

How many bytes does that take up? Let's say one byte per token compressed, so a few hundred bytes per item.

How many training items are there for a dataset like Alpaca? 52K.

So how big is that? 52K (Q&A pairs) * ~300 (bytes per pair) = 15.6 MB

How long does it take to upload 15.6MB over a cloud internet connection, e.g. 1 Gbs? = 0.124 seconds

How long does it take to train a 70B parameter model on that data on an H100 at 40% MFU? = 6.5 hours 

So the upload perf isn't nearly as big of a practical problem as it seems.

We built a fast file uploader in Cray-LM that can handle 10s to 100s of GBs of data over common cloud
connections. It hashes the data to avoid uploading duplicates. No need for an object store.

Our main insight is as follows. If you are running Cray-LM, you built a supercomputer capable of training an
LLM. That computer can handle moving around a few GB, so could most laptops today. An object store managing
thousands of parallel disks is overkill.

### Future

We plan to keep Cray-LM up to date with the latest advances in LLM research, including new models,
new training stack optimizations, and new inference stack optimizations.

We are excited to build a new generation of LLM supercomputers that democratize access to
training language models. We are in particular interested in applications that build on
top of Cray-LM to support LLM agents that train and deploy themselves.

We welcome community contributions. Some areas that could use help:
1. A [Llama Stack](https://llama-stack.readthedocs.io/en/latest/introduction/index.html) client
2. An [AILuminate](https://ailuminate.mlcommons.org/benchmarks) eval
3. Performance optimizations in the training and inference engines
4. Deployment targets, benchmarks, and regression tests


